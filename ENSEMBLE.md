# Ensemble Prediction Architecture

## Background

In Time-Dependent Density Functional Theory (TDDFT), the electronic density matrix $\rho(t)$ contains elements with vastly different behaviors:
1.  **High-Variance Elements:** Often diagonal elements (populations) or strong coherences that oscillate significantly under an external field.
2.  **Low-Variance Elements:** Often off-diagonal elements representing weak coherences or "quiet" interactions that remain close to zero or oscillate with very small amplitudes.

**The Problem:**
When training a single neural network (ResNet) on the entire matrix, the loss function (MSE) is dominated by the high-variance elements. The network prioritizes learning the "loud" features to minimize global error. Consequently, the "quiet" elements are often neglected, leading to unphysical drift or immediate divergence during autoregressive inference (bootstrapping).

## The Logic: Divide and Conquer

The **Ensemble Approach** addresses this by decoupling the learning process for different scales of dynamics.

### Architecture
The `EnsembleDensityResNet` is a container module that manages multiple independent sub-models (`DensityResNet` instances).

1.  **Partitioning (Masks):** The density matrix indices $(s, i, j)$ are partitioned into $K$ disjoint subsets (bins). Each bin has a corresponding binary mask $M_k$.
2.  **Specialization:** A dedicated sub-model $f_k$ is assigned to each bin.
3.  **Full Context:** Every sub-model receives the **full** input history and field information. This ensures that a "quiet" element can still react to changes in "loud" elements (coupling is preserved).
4.  **Masked Output:** The output of sub-model $k$ is multiplied by mask $M_k$.
5.  **Aggregation:** The final prediction is the sum of masked outputs:
    $$ \rho_{pred} = \sum_{k=1}^{K} (f_k(\text{input}) \odot M_k) $$

By training this ensemble end-to-end, gradients for sub-model $k$ are only generated by errors in the elements belonging to mask $M_k$. This prevents high-variance elements from affecting the weight updates of the model responsible for low-variance elements.

## Usage

The ensemble behavior is controlled entirely through the JSON configuration file passed to `scripts/train.py`.

### 1. Automatic Variance-Based Binning

This is the recommended default. The training script analyzes the training data, calculates the variance of every matrix element, and groups them into quantiles.

**Config (`configs/h2p_ensemble.json`):**
```json
{
  "model": {
    "type": "EnsembleDensityResNet",
    "n_bins": 2, 
    "max_nbf": 4,
    "hidden_dim": 64,
    "num_resnet_blocks": 4,
    ...
  }
  ...
}
```
*   `type`: Must be `"EnsembleDensityResNet"`.
*   `n_bins`: The number of sub-models to create. `2` is usually sufficient (High Var vs. Low Var).

### 2. Manual Binning

If you want specific control over which elements are grouped together (e.g., grouping specific physical interactions), you can explicitly list the indices.

**Config:**
```json
{
  "model": {
    "type": "EnsembleDensityResNet",
    "bins": [
        [ [0,0,0], [0,1,1] ],  // Bin 0: Specific diagonal elements
        [ [0,0,1], [0,1,0] ]   // Bin 1: Specific off-diagonal pair
    ],
    ...
  }
  ...
}
```
*   `bins`: A list of lists. Each inner list contains `[spin, row, col]` indices.
*   **Automatic Remainder:** If your manual bins do not cover every element in the matrix, the system automatically creates an extra "Catch-all" bin (and an extra sub-model) for the remaining elements.

## Training and Inference

**Training:**
```bash
python scripts/train.py --config configs/h2p_ensemble.json
```
The script will print the bin thresholds (for automatic mode) or the coverage counts (for manual mode) during initialization.

**Inference:**
```bash
python scripts/predict.py --checkpoint test/h2p/checkpoints_ensemble/best_model.pt ...
```
The `DensityPredictor` automatically detects that the checkpoint contains an ensemble model. The masks used during training are saved within the checkpoint, ensuring the exact same partitioning is used during inference.
